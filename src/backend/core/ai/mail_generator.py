import os
import json
import math
import random
from typing import Optional
from core.models import Thread, PromptEvaluation
from core.services.ai_service import AIService
from core.ai.thread_summarizer import get_messages_from_thread, Message
from django.db.models import Avg, Count, IntegerField
from django.db.models.functions import Cast

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROMPTS_FILE_PATH = os.path.join(BASE_DIR, "ai_prompts.json")
with open(PROMPTS_FILE_PATH, "r") as f:
    AI_PROMPTS = json.load(f)


def select_prompt(prompt_type: str = None, lambda_rarity: float = 1.0):
    if prompt_type:
        filtered_prompts = [p for p in AI_PROMPTS if p["type"] == prompt_type] if prompt_type else AI_PROMPTS
    
    # Récupération des stats
    stats = PromptEvaluation.objects.filter(
        prompt_type=prompt_type
    ).values("prompt_id").annotate(
        like_ratio=Avg(Cast("accepted", IntegerField())),
        total=Count("id")
    )

    # Mapping des stats
    stats_map = {
        row["prompt_id"]: {
            "like_ratio": row["like_ratio"] or 0.0,
            "total": row["total"]
        }
        for row in stats
    }

    # Calcul des scores
    scored_prompts = []
    for prompt in filtered_prompts:
        prompt_id = prompt["id"]
        stats = stats_map.get(prompt_id, {"like_ratio": 0.0, "total": 0})
        ratio = stats["like_ratio"]
        total = stats["total"]

        score = (ratio * math.log(1 + total)) + (lambda_rarity / (1 + total))
        scored_prompts.append((prompt, score))

    # Tirage pondéré
    total_score = sum(score for _, score in scored_prompts)
    rand = random.uniform(0, total_score)
    cumulative = 0.0

    for prompt, score in scored_prompts:
        cumulative += score
        if rand <= cumulative:
            return prompt["content"], prompt["id"]
    return scored_prompts[-1][0]["content"], scored_prompts[-1][0]["id"]


def generate_answer_mail(thread: Optional[Thread], draft: str, prompt: str, name: str) -> str:
    """
    Generates an answer to a thread using the ALBERT model.
    Args:
        thread (Thread): Thread to answer.
    Returns:
        str: Answer to the thread.
    """

    # Prepare the prompt for the AI model
    messages = get_messages_from_thread(thread) if thread else []
    conversation_text = "\n\n".join([str(message) for message in messages]) if messages else ""
    
    selected_prompt, prompt_id = select_prompt("answer_mail")

    role = selected_prompt["role"]
    answer_rules = selected_prompt["answer_rules"]
    global_prompt_template = selected_prompt["global_prompt_template"]
    example = selected_prompt["example"]

    global_prompt = global_prompt_template.format(
        role=role,
        answer_rules=answer_rules,
        thread_context=conversation_text,
        example=example,
        draft=draft,
        name=name
    )
    answer = AIService().call_ai_api_with_extra_instructions(global_prompt, prompt)
    return {
        "answer": answer,
        "prompt_id": prompt_id
    }


def generate_new_mail(draft: str, prompt: str, name: str) -> str:
    """
    Generates a new mail using the ALBERT model.
    Args:
        draft (Str): Message draft.
        prompt (Str): user prompt for the AI assistant.
    Returns:
        str: Message generated by the AI assistant.
    """
    selected_prompt, prompt_id = select_prompt("new_mail")

    role = selected_prompt["role"]
    answer_rules = selected_prompt["answer_rules"]
    global_prompt_template = selected_prompt["global_prompt_template"]
    example = selected_prompt["example"]

    global_prompt = global_prompt_template.format(
        role=role,
        answer_rules=answer_rules,
        example=example,
        draft=draft,
        name=name
    )
    answer = AIService().call_ai_api_with_extra_instructions(global_prompt, prompt)
    return {
        "message": answer,
        "prompt_id": prompt_id
    }